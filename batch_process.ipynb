{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_P7MABFq_2A9",
    "outputId": "9d67c6b7-18f2-491b-9a79-2da532fc8016"
   },
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0️⃣ Install / upgrade required packages\n",
    "# (Run this cell first in Colab Pro)\n",
    "!pip install -q --upgrade transformers accelerate bitsandbytes torch\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUz58lcWAXsz",
    "outputId": "8e9281e0-4dff-4704-fb2d-dd7b4d4b7ba9"
   },
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 1️⃣ Imports & device\n",
    "import os, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# If you need to authenticate for private or gated models:\n",
    "token = os.getenv(\"HF_TOKEN\")  # ✅ much safer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "9b12e3e6afd14c08aaf0fc2d8fce3c05",
      "53205f84c5754f209d195672ea38c4bd",
      "403cd8d0b21d46af9b01be65b6ff7f3a",
      "0a4e2d7c67c4447c9e1cc0102a6bafdd",
      "d43021b5c5574f0a8e858553b4f6405d",
      "34cb0488b75c43718e1cdb8ca0490d96",
      "315211d1eedb407da63ae20a9bfc64d5",
      "163927de52a04cf2a5e4c63397ca4d43",
      "c768b87dd93240f3bf83d9e275872325",
      "50f67d12313341f3b66f92c0909a65d0",
      "c1d59829f53a4b76b699abb2a7662ae7"
     ]
    },
    "id": "XW-4t9KzBULc",
    "outputId": "32ce38c5-ab34-4454-eff3-cc9bfd8e85fb"
   },
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 2️⃣ Load & slice one year of data (8 640 hourly temps)\n",
    "df = (\n",
    "    pd.read_csv(\"weatherHistory.csv\", parse_dates=[\"Formatted Date\"])\n",
    "      .sort_values(\"Formatted Date\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "FEATURE = \"Temperature (C)\"\n",
    "LOOKBACK = 8640   # 1 year of hourly samples\n",
    "HORIZON  = 720    # 30 days × 24 h\n",
    "\n",
    "temps = df[FEATURE].values[-LOOKBACK:]\n",
    "assert temps.shape[0] == LOOKBACK\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 3️⃣ Build the direct multi‑step prompt\n",
    "# WARNING: this is very long—ensure your context size can hold ~8640 tokens + space for 720 outputs.\n",
    "temps_list = [round(float(x),1) for x in temps]\n",
    "# join without spaces to save tokens; you can also downsample if you hit truncation\n",
    "input_str = \",\".join(str(x) for x in temps_list)\n",
    "\n",
    "prompt = (\n",
    "    f\"Given the past {LOOKBACK} hourly temperatures: [{input_str}], \"\n",
    "    f\"predict the next {HORIZON} hourly temperatures. \"\n",
    "    \"Reply with a comma‑separated list of numbers.\"\n",
    ")\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 4️⃣ Load Mistral‑7B FP16 via Accelerate\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "token = os.environ[\"HUGGINGFACE_HUB_TOKEN\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\"   # ⚠️ this loads everything into GPU\n",
    ")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 5️⃣ Tokenize, generate, and time it\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=8192  # adjust if you have a longer context window\n",
    ").to(device)\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=4096,   # allow space for up to ~4k tokens of answers\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "elapsed = time.time() - start\n",
    "print(f\"Single-shot generation took {elapsed:.1f} seconds\")\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 6️⃣ Decode & parse out 720 floats\n",
    "resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "nums = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", resp)\n",
    "preds = [float(x) for x in nums[:HORIZON]]\n",
    "\n",
    "print(f\"Parsed {len(preds)} hourly predictions:\")\n",
    "print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0T1oJd_2J6ha",
    "outputId": "7154a91d-6789-48f5-bf1a-be2676640840"
   },
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "ccLRtRNfBgni",
    "outputId": "d9b6994a-f3e8-4dad-9a53-d5f8994d27c9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert to numpy arrays\n",
    "actual = np.array(next_month_actual)\n",
    "preds = np.array(llm_preds)\n",
    "\n",
    "# Remove extreme outliers from preds (e.g., values > 100 or < -50)\n",
    "preds_cleaned = np.clip(preds, -50, 100)\n",
    "\n",
    "# Time index\n",
    "hours = np.arange(len(actual))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(hours, actual, label=\"Actual Temperatures\", color='blue')\n",
    "plt.plot(hours, preds_cleaned, label=\"LLM Forecast (Mistral-7B)\", color='orange', linestyle='--')\n",
    "plt.xlabel(\"Hour (from forecast start)\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.title(\"30-Day Hourly Temperature Forecast: LLM vs Actual\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "JEVux80FD0K4",
    "outputId": "ce432008-067d-4646-9dab-479a72f27a4a"
   },
   "outputs": [],
   "source": [
    "# Ensure predictions are safe for plotting\n",
    "llm_preds = np.array(llm_preds)\n",
    "llm_preds_clipped = np.clip(llm_preds, -50, 100)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(hours, next_month_actual, label=\"Actual Temperatures\", color='blue')\n",
    "plt.plot(hours, llm_preds_clipped, label=\"LLM Forecast (Mistral-7B)\", color='orange', linestyle='--')\n",
    "plt.xlabel(\"Hour (from forecast start)\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.title(\"30-Day Hourly Temperature Forecast: LLM (Mistral-7B) vs Actual\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9DxMlE1EFKx"
   },
   "outputs": [],
   "source": [
    "!pip install prophet -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijsBLplTJJUN",
    "outputId": "afa6c900-1d31-40ce-e751-8c0d422a2bb4"
   },
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "\n",
    "# Load and sort the dataset\n",
    "df = pd.read_csv(\"weatherHistory.csv\", parse_dates=[\"Formatted Date\"])\n",
    "df = df.sort_values(\"Formatted Date\").reset_index(drop=True)\n",
    "\n",
    "# Step 1: Ensure datetime is parsed correctly and convert to UTC first\n",
    "df[\"Formatted Date\"] = pd.to_datetime(df[\"Formatted Date\"], utc=True)\n",
    "\n",
    "# Step 2: Then remove timezone (convert to naive datetime)\n",
    "df[\"Formatted Date\"] = df[\"Formatted Date\"].dt.tz_localize(None)\n",
    "\n",
    "# Step 3: Prepare dataframe for Prophet\n",
    "df_prophet = df[[\"Formatted Date\", \"Temperature (C)\"]].rename(\n",
    "    columns={\"Formatted Date\": \"ds\", \"Temperature (C)\": \"y\"}\n",
    ")\n",
    "\n",
    "# Split data into train and test (last 720 hours as test set)\n",
    "train_df = df_prophet[:-720]\n",
    "test_df = df_prophet[-720:]\n",
    "\n",
    "# Step 4: Fit Prophet model\n",
    "model = Prophet()\n",
    "model.fit(train_df)\n",
    "\n",
    "# Step 5: Forecast next 720 hours\n",
    "future = model.make_future_dataframe(periods=720, freq='H')\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Step 6: Extract predictions\n",
    "prophet_preds = forecast[-720:][\"yhat\"].values\n",
    "next_month_actual = test_df[\"y\"].values\n",
    "\n",
    "print(\"✅ Forecast complete. Prophet predicted\", len(prophet_preds), \"values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NKPWzbUrLD8B",
    "outputId": "82279d0e-c081-4465-cfa4-2e2eb022ced7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create time axis\n",
    "time = np.arange(720)\n",
    "\n",
    "# Clean LLM predictions (replace with your actual LLM output variable if needed)\n",
    "llm_preds_array = np.array(llm_preds)\n",
    "llm_preds_cleaned = np.clip(llm_preds_array, -50, 100)  # clip outliers for safe plot\n",
    "\n",
    "# ========== 📊 PLOT 1: Prophet vs Actual ==========\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(time, next_month_actual, label=\"Actual\", color=\"black\")\n",
    "plt.plot(time, prophet_preds, label=\"Prophet\", color=\"green\", linestyle=\"--\")\n",
    "plt.title(\"📈 Prophet vs Actual Temperature Forecast\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== 📊 PLOT 2: LLM vs Actual ==========\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(time, next_month_actual, label=\"Actual\", color=\"black\")\n",
    "plt.plot(time, llm_preds_cleaned, label=\"Mistral LLM\", color=\"orange\", linestyle=\"--\")\n",
    "plt.title(\"📈 LLM vs Actual Temperature Forecast\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== 📊 PLOT 3: Prophet vs LLM vs Actual ==========\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(time, next_month_actual, label=\"Actual\", color=\"black\")\n",
    "plt.plot(time, prophet_preds, label=\"Prophet\", color=\"green\", linestyle=\"--\")\n",
    "plt.plot(time, llm_preds_cleaned, label=\"Mistral LLM\", color=\"orange\", linestyle=\":\")\n",
    "plt.title(\"📈 Comparison: Prophet vs LLM vs Actual\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== 📐 METRICS ==========\n",
    "def print_metrics(name, y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Manual RMSE\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n🔎 {name} Metrics:\")\n",
    "    print(f\"  RMSE : {rmse:.4f}\")\n",
    "    print(f\"  MAE  : {mae:.4f}\")\n",
    "    print(f\"  R²   : {r2:.4f}\")\n",
    "\n",
    "print_metrics(\"Prophet\", next_month_actual, prophet_preds)\n",
    "print_metrics(\"Mistral LLM\", next_month_actual, llm_preds_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-pYbna2L4P_",
    "outputId": "ddb8ddc4-d697-4ef6-a3dd-4d6ffd0350b3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame with hourly index\n",
    "hours = list(range(720))\n",
    "\n",
    "# Save predictions and actual values\n",
    "results_df = pd.DataFrame({\n",
    "    \"Hour\": hours,\n",
    "    \"Actual_Temp\": next_month_actual,\n",
    "    \"Prophet_Pred\": prophet_preds,\n",
    "    \"LLM_Pred\": llm_preds_cleaned\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(\"temperature_forecast_comparison.csv\", index=False)\n",
    "print(\"✅ Saved to temperature_forecast_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cevW2vAAMzv8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
